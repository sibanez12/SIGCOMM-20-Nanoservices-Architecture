\section{Design Considerations}
Our \name{} is deliberately made simple, in part because of the constraints of an academic research project. The designer of a complete \name{} will need to consider many additional aspects of the design. We address some of the key tradeoffs. 

\paragraph{GPRs vs CSRs} Our \name{} prototype on the RISC-V Rocket core repurposes two of its 32 general-purpose registers (GPRs) for the head and tail of the network queues. In a CPU with 32 registers, we can likely afford to lose two; our benchmark nanoservice applications did not suffer from the loss of two GPRs. However, if they did, the cost would be high as variables would spill over into memory, increasing latency. This would be exacerbated in embedded applications for a CPU with fewer GPRs. We therefore experimented with a design where the head and tail registers are implemented using control status registers (CSRs) instead, which might need to be considered in  settings with limited GPRs.

\paragraph{Floating point} Our \name{} design minimizes the latency from the network into the \emph{integer} register file.
Many scientific computing applications, such as the N-body simulation, rely on \emph{floating point} arithmetic, with their own set of floating point GPRs.
The default RISC-V ISA does not include instructions to copy an integer register to a floating point register; the value must first be stored into memory and then loaded into the floating point register file, increasing latency. If necessary, a designer should add instructions to the RISC-V ISA to copy between the two types of GPR. 

\paragraph{In-order Execution} Our \name{} prototype is built from a simple 5-stage, in-order RISC-V Rocket core.  While our prototype required very minor modifications to the CPU pipeline, the changes would require more invasive changes to an out-of-order processor in order to ensure that words are read from the RX queue in FIFO order. This is even evident on our simple 5-stage core: if there is a branch mis-prediction when reading the network RX queue, the CPU pipeline's decode stage causes a state change which must be undone when the pipeline is flushed. We modified the RX queue to ``unread'' the last two words that were removed. A more complete design would be needed for a CPU with out-of-order, speculative execution.

%\paragraph{In-order execution} The \name{} prototype is our first step towards building a domain specific-processor for nanoservice applications requiring extremely fine-grained parallelism.  So far, we focused on, what we believe to be, the critical latency path: the network interface and thread scheduler, along with the accompanying changes to the compiler and assembler.  

\paragraph{Domain specific versus general purpose} For clarity, we describe \name{} here as if it only runs nanoservices. This might be ideal, particularly if many applications can exploit its low-latency design. We can imagine a single chip with hundreds or thousands of \name{} cores, with little or no cache memory. But in other settings, for example in a warehouse computer, it might be necessary (or economically prudent) to support general purpose workloads too. As-is, \name{} does not support \emph{memory-intensive} distributed applications, such as a Key-Value store~\cite{memcached} (unless the database can be sharded so small that it fits in the L1 cache). In this case, the \name{} optimizations might be added to an otherwise general-purpose CPU. The NIC, for example, could steer only nanotask messages to the GPRs, and steer memory intensive traffic (e.g. RDMA or regular TCP) to the traditional DMA, memory, and cache path. This would require a more sophisticated thread scheduler for very low latency.

\paragraph{Domain specific languages} We assume that the NIC pipeline is programmed using the P4 language, or similar, for fast header processing. It is worth considering whether C is the correct language for nanotasks: Perhaps a set of libraries, restricted C, or a special language could help programmers craft very fast nanotasks using explicit message processing.

\paragraph{Protection and security} The J-machine~\cite{jmachine} first used GPRs for low-latency communication, but the approach was abandoned because it was felt that isolating multiple contexts running on the same core required too much hardware software. With many generations of improvement in process technology (from $10^6$ transistors per chip in 1989 to over $10^10$ today), the per-context queues in the \name{} solve this problem in a manner not feasible at the time of the J-machine. 

\paragraph{Kernel} Our prototype replaces Linux with our own minimal operating system, the nanokernel, designed to minimize average and peak latency for context switches. Our nanokernel currently lacks virtual memory and different privilege modes;
applications and the nanokernel currently execute in the same address space and at the same privilege level. Our next generation nanokernel will address these issues, and we do not anticipate much increase in latency. A real system might also need a runtime resource scheduler to ensure high utilization of \name{} cores and NICs by dispatching nanoservice apps to cores, monitoring the utilization of the cores and NICs, the status of contexts, adjusting their priorities, and rebalancing over/under-utilized \name{}s on a longer timescale. 




% \begin{itemize}
%     \item The number of nanoservice apps running on any one core will likely be small in order to ensure that all the code and data resides in on-chip SRAM. Thus, we believe that it is feasible for the NIC to maintain per-context queues in hardware.
%     \item Mention that our prototype will be open sourced and all of our results will be made reproducible.
% \end{itemize}