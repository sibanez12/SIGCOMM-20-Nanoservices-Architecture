\subsection{Terminating Transport in the NIC}
\label{ssec:nic-transport}
If the \name{} is to meet our aggressive latency target of $<1\mu$s from when an RPC message arrives over Ethernet until a response is sent back out, there is clearly no time to run a traditional transport networking stack in software. Therefore, \name{} runs a low-latency transport protocol in hardware, in the NIC, with support from the NIC's programmable pipeline. Note that transport protocols for low-latency applications (e.g., DCQCN for RDMA) are already fully implemented in hardware~\cite{cvl,connectx6} on state-of-the-art high-speed NICs available in today's market, vouching for the feasibility of a fully-offloaded transport protocol running in hardware.

We, however, don't mandate a specific transport protocol in this paper because we seem to be entering an era when different cloud providers prefer different transport protocols~\cite{timely,hpcc,dcqcn}. Instead we aim to provide some choices, within our tight latency budget.

The NIC therefore places minimal constraints on the transport protocol, while providing programmable hardware blocks to allow some choice by the network owner. We assume that the transport layer provides a reliable message abstraction to each thread, as well as network congestion control. It therefore must handle retransmissions and decide when packets should be sent. To guide our design, we assume it must be possible for the NIC to be programmed to implement Homa~\cite{homa} and NDP~\cite{ndp}. Between them, they mandate most of the building blocks we need, including reliable transmission, immediate startup rate, receiver driven scheduling, a message abstraction to the CPU, and data-trimming (in the switches).

We observe that realizing a transport protocol in hardware requires the following functions in the programmable pipeline of a NIC.

\begin{itemize}[topsep=0.4\baselineskip, leftmargin=20pt]
    \item Timers and timer-based event processing logic to realize various timer-based state transitions, such as retransmissions and timeouts.
    \item Pacers to rate-limit individual flows.
    \item State machines to maintain per-flow state, including current rate or congestion-window size, sequence and ack numbers, connection status, counters, etc.
    \item Packetization/retransmission buffer to break a message into packets and hold packets until they are acknowledged by a receiver.
    \item Reordering buffer to handle out-of-order packets.
    \item Packet generators to realize receiver-driven transport protocols that keep generating credit packets for senders.
\end{itemize}

The event-driven PISA pipeline~\cite{event-driven-pisa} already provides most of the mechanisms we need for sophisticated stateful operations (\eg, state machines, timer events and packet generation), and can be extended to add support for message reassembly and retransmit buffers~\cite{schuehler2004modular, intel-rapidio}. Because nanoservice messages sizes will be very small, the amount of SRAM needed to realize these buffers will be sufficiently small for a cost- and power-efficient hardware implementation. With these mechanisms, we believe the \name{} can run a complete transport stack on the NIC with very low latency. While we have a design for each of these building blocks, we will explain the details of those in a follow-up paper.
