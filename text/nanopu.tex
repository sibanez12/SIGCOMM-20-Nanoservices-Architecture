\section{The \name{}}
\label{sec:nanoPU}
The \name{} is a new domain-specific processor optimized for running nanotasks---quickly and predictably---for compute-intensive distributed applications based on nanoservices. 
A \name{} consists of one or more CPU cores and one or more low-latency NICs. 
The CPU cores are slightly-modified cores; our design is based on the popular, open-source RISC-V ISA~\cite{riscv}. 
The low-latency NIC is inspired by the recently proposed {\em Lightning NIC (L-NIC)}~\cite{lnic}, a novel approach that terminates the transport layer in hardware, then delivers message data right into the registers at the heart of the CPU core.  
The L-NIC approach minimizes latency (and unpredictability) by bypassing DMA, cache, and DRAM entirely. 
Our \name{} design also adds a novel hardware thread scheduler, to minimize the time (and variability) from when a message arrives until the thread starts processing it.

Figure~\ref{fig:nanoPU} shows a high-level block diagram of a \name{}. 
This particular example shows one network interface shared by three cores, but in general the \name{} is designed to work with any ratio of NICs to cores. 
Depending on the context, it may make sense to build \name{}s with one core per NIC (\eg, for small embedded systems), all the way to hundreds of cores per NIC. 
For example, it would be practical today to design a chip with 500 cores~\cite{celerity, kilocore} and over a hundred \SI{100}{Gb/s} Ethernet interfaces;\footnote{Commercial switch chips exist with $128\times 100$ Gb/s Ethernet MACs today.} in this example, the ratio would be five cores per NIC. 
Of course, many other ratios are possible and the ideal ratio depends on the application, technology, and economics. 

The \name{} is a domain-specific processor. 
With the slowing of Moore's Law, new domain-specific processors are being widely used as accelerators for specific, high-volume workloads, such as graphics~\cite{nvidia-geforce}, machine-learning~\cite{tensorflow} and networking~\cite{RMT}. 
While the \name{} is not nearly as radical a departure from a general-purpose CPU as, say, a GPU, TPU or programmable switch (after all, our design relies heavily on an existing core), the \name{} shares the approach of tailoring the chip design for a specific class of applications.\footnote{As an aside, it is only possible to consider prototyping a \name{} in a university because of two recent trends: RISC-V provides a remarkably stable starting point for heavy-lifting, and the narrow performance gap between the leading edge process (currently \SI{7}{nm}) and the closest already-paid-for process (\SI{16}{nm}) makes it economically feasible to build an interesting \name{}.}

If \name{} is so fast, why are not all CPUs designed this way? 
It is because general-purpose CPUs are optimized for general-purpose workloads, most of which are memory-intensive. 
They are ``load-store'' machines, with memory as a first-class citizen. 
Arriving and departing packet data must pass through the memory subsystem first, on its way in and out of the CPU. 
Our approach is, instead, to make network messages first-class citizens in their own right, independent of memory. 
Network messages arrive directly into the CPU, without needing to make their way through the memory hierarchy. 
General-purpose CPUs put memory in the network and attach compute to memory, which makes perfect sense for memory-intensive applications. 
Our approach in the \name{} is to tightly couple compute directly to the network, and then attach memory on the side as needed.

The \name{} has the following key characteristics that we visit, in turn, in the next few subsections: 
\begin{itemize}
    \item {\bf NIC Datapath:} A programmable event-driven PISA ``Match-Action Unit'' (MAU) pipeline~\cite{event-driven-pisa} on the NIC to process packet headers as they arrive and depart over the network, terminate tunnels, encrypt/decrypt and compress/decompress data; and a low-latency transport protocol in hardware, such as Homa~\cite{homa} or NDP~\cite{ndp}. The NIC Datapath presents a packet interface to the NIC Message Reassembler and Packetizer.\shahbaz{this is too much info at this point; also I'd recommend not empahsizing things that we haven't built.}
    
    \item {\bf NIC Message Reassembler and Packetizer:} Hardware FIFOS programmatically bound to each running nanotask CPU context that handle the reassembly of inbound packets into messages and the packetization of outbound messages into packets. This allows the NIC Message Reassembler and Packetizer to present a message interface to each nanotask executing on the CPU.\shahbaz{the NIC-Core interface seemed better, as it relates with figure and para headings? also the statements here are more verbose now.}
    
    \item {\bf Hardware Thread Scheduling:} The NIC ensures that arriving message data reaches the core quickly. We also need to make sure the core switches to the message's nanotask recipient to process the message quickly; to support this requirement, the hardware includes a very low-latency thread scheduler~\cite{resq}.
\end{itemize}

These characteristics together yield a very low latency path---just a few clock cycles---from the network right to the very heart of the CPU core---its register file, giving mean and tail latency reduction of one to two order of magnitude. We do not believe there is a lower latency path to a running thread.

\begin{figure}
  \includegraphics[width=0.9\linewidth]{./figures/nanopu-arch}
  \caption{A high-level architecture of \name{} having a NIC datapath with an event-driven MAU pipeline, a NIC-Core Interface with RX/TX context FIFOs per core and a thread scheduler, as well as CPU cores running the nanoKernel and nanotasks.}
  \label{fig:nanoPU}
\end{figure}

\alex{The nanoPU layer we're calling "NIC-Core interface" might be easier to understand if we refer to it as something like the "NIC Message Reassembler and Packetizer" layer. Since right now we have to keep referring to "the interface between the NIC-Core interface and the CPU", which is a little hard to follow. I'm wondering if it might be easier to have a separate name for the hardware (i.e. the per-context FIFOs, reassembly, packetization, and scheduling) -- then the NIC-core interface really is just the interface (i.e. the GPRs, CSRs and message formats.}

\input{text/nic-datapath}
\input{text/nic-core-interface}
\input{text/nic-scheduler}
\input{text/nic-transport}
