\section{Conclusion}
\nick{Please review conclusion.}
There are several conclusions to draw from this extreme approach to distributed computing. On one hand, the \name{} suggests a fairly simple, low-cost, non-invasive way to modify a CPU+NIC to accelerate distributed applications that can exploit very fine-grained nanotasks. We could expect dramatic speedups for this class of applications. On the other hand, one might dismiss nanoservices as too fine-grained for the remaining broad class of applications that require accesses to large pools of memory; these are unlikely to benefit from nanoservices because they cannot be cache resident. We envisage three deployment scenarios: First, racks of \name{}s in an otherwise unchanged datacenter, to which nanoservice jobs are directed. A single chip might contain over a thousand \name{} cores sharing over a hundred on-chip low-latency L-NIC interfaces. This would be a formidable platform for running nanoservice applications. Further, one can imagine extending stream processing and load-balancing into the NIC pipeline and network switches, offloading the CPU cores from processing tasks better suited to in-network computation. The second scenario is where an existing CPU has the \name{} features added to it, in addition to its normal cache, DRAM, DMA and PCIe hardware. In this case, the NIC would steer nanoservice RPC messages directly to the registers, while steering legacy network traffic via DMA into memory. A third scenario is for embedded applications, for example the CPUs on a modern smart NIC. These could be designed as a \name{} either to accelerate control of the smart NIC, or to host nanoservices to offload the host server. All three scenarios are technically possible; it remains to be seen if there is sufficient demand for nanoservices to make any of them worthwhile. 
