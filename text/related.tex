\section{Related Work}

\paragraph{Building Distributed Apps}
Microservices and serverless compute is growing more in popularity for finer-grained computing than in the past~\cite{aws-lambda, gcloud-functions, azure-functions}, and has been used to implement fine grained video encoding~\cite{ExCamera}, compilers~\cite{gg}, and scientific computing applications~\cite{PyWren}.
Our nanoservices framework would drive this approach to the extreme. Our \name{} is designed to make it possible.

\paragraph{Minimizing RPC Tail Latency}
Several authors have recently described novel approaches to minimize tail RPC completion time (Shinjuku~\cite{shinjuku}, Shenango~\cite{shenango}, and RPCValet~\cite{rpcvalet}) by efficiently load balancing incoming requests across cores. While demonstrably good for {\em micro}services, software schedulers are too slow for nanoservices. We believe load balancing messages to nanotasks is best done by the sender or by the network, not the receiving host where it is already too late, especially for the massive degrees of incast that we expect nanoservices to exhibit. Fine-grained load-balancing across \name{}s, plus receiver-driven scheduling in the transport layer (a la Homa~\cite{homa} and NDP~\cite{ndp}) should balance load better than a software scheduler could at the receiver. 

\paragraph{Global Clock Synchronization} If \name{}s have tightly synchronized clocks (within say 100ns), like SIMON~\cite{10.5555/3323234.3323280}, PTP~\cite{rfc8173}, or DTP~\cite{dtp}, the sender can make informed load balancing decisions, or delay sending an RPC to avoid queues and drops. This is now much easier to deploy with the advent of per-packet INT~\cite{INT} with nanosecond resolution to eliminate noisy synchronization signals caused by queueing delay.  

\paragraph{NIC Hardware}
Smart NICs (NICs with CPUs on them) are becoming more popular, particularly for shared cloud data centers~\cite{nitro, bluefield, pensando}. They aim to offload infrastructure software from the revenue-generating host CPUs onto smaller, lower power CPUs on the NIC. These designs are likely to increase latency for nanoservices, unless they contain a dedicated \name{}-like fastpath for RPC messages on the NIC's data path. Alternatively, the CPUs on the NIC could include \name{}s, to provide low latency RPCs without bothering the main host CPU. 

\paragraph{Integrated NIC} We are not the first to propose an integrated network interface.
Scale-out NUMA~\cite{scale-out-numa} accelerates RDMA-style applications by integrating the NIC into the machine's local cache-coherence hierarchy. The recent Compute Express Link (CXL)~\cite{cxl} standard is similar; it maintains coherence between the CPU memory and the memory on PCIe-attached devices. These approaches, while good for throughput, are not optimal for nanoservices, which require minimal latency into the CPU, not memory.


